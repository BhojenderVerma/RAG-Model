import streamlit as st
import time
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema.document import Document
from langchain_ollama import OllamaEmbeddings
from langchain_ollama import OllamaLLM
from langchain_chroma import Chroma

# ---- Load and Prepare Data (run once at startup) ----

@st.cache_resource
def setup_db():
    loader = PyPDFDirectoryLoader("data/")
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=80,
        length_function=len,
        is_separator_regex=False
    )
    chunks = text_splitter.split_documents(documents)
    # Assign unique IDs
    last_page_id = None
    current_chunk_index = 0
    for chunk in chunks:
        source = chunk.metadata.get("source")
        page = chunk.metadata.get("page")
        current_page_id = f"{source}:{page}"
        if current_page_id == last_page_id:
            current_chunk_index += 1
        else:
            current_chunk_index = 0
        chunk.metadata["id"] = f"{current_page_id}:{current_chunk_index}"
        last_page_id = current_page_id
    # Create Chroma DB
    embedding = OllamaEmbeddings(model="nomic-embed-text")
    db = Chroma(
        persist_directory="./chroma_db",
        embedding_function=embedding,
        collection_name="gita_collection"
    )
    existing_items = db.get(include=[])
    existing_ids = set(existing_items["ids"])
    new_chunks = [chunk for chunk in chunks if chunk.metadata.get("id") not in existing_ids]
    new_ids = [f"doc_{len(existing_ids) + i}" for i in range(len(new_chunks))]
    if new_chunks:
        db.add_documents(documents=new_chunks, ids=new_ids)
    return db

db = setup_db()

# ---- Streamlit Web UI ----

st.title("Ask Anything (RAG with Ollama)")
st.write("Enter your question below. The answer will be generated by your local AI model using your PDF knowledge base.")

query_text = st.text_input("Your Question:", "")

if st.button("Ask") and query_text.strip():
    start_time = time.time()
    # Retrieve relevant chunks
    results = db.similarity_search_with_score(query_text, k=5)
    context_text = "\n\n---\n\n".join([doc.page_content for doc, _ in results])
    PROMPT_TEMPLATE = """
    Answer the question based only on the following context:
    {context}
    Answer the question based on the above context: {question}
    """
    prompt = PROMPT_TEMPLATE.format(context=context_text, question=query_text)
    # Call Ollama LLM
    model = OllamaLLM(model="llama3")
      # Change to your preferred local model
    with st.spinner("Thinking..."):
        response = model.invoke(prompt)
        st.markdown("## Answer")
        st.write(response)
        st.markdown("---")
        st.markdown("**Sources:**")
        for doc, _ in results:
            st.write(f"Source: {doc.metadata.get('source', 'Unknown')} | Page: {doc.metadata.get('page', 'Unknown')}")
        st.write(f"⏱️ Time taken: {time.time() - start_time:.2f} seconds")

st.info("Make sure Ollama is running (`ollama serve`) and the model is downloaded.")

